{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import BinaryEncoder, OneHotEncoder, TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OrdinalEncoder\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import joblib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data number = 25000\n",
      "Test data number = 10000\n",
      "\n",
      "Index(['listing_id', 'title', 'make', 'model', 'description', 'manufactured',\n",
      "       'original_reg_date', 'reg_date', 'type_of_vehicle', 'category',\n",
      "       'transmission', 'curb_weight', 'power', 'fuel_type', 'engine_cap',\n",
      "       'no_of_owners', 'depreciation', 'coe', 'road_tax', 'dereg_value',\n",
      "       'mileage', 'omv', 'arf', 'opc_scheme', 'lifespan', 'eco_category',\n",
      "       'features', 'accessories', 'indicative_price', 'price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_data = pd.read_csv('Data/train.csv')\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "print('Training data number = {}'.format(train_data.shape[0]))\n",
    "print('Test data number = {}\\n'.format(test_data.shape[0]))\n",
    "print(train_data.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_train_data: Index(['depreciation', 'dereg_value', 'manufactured', 'coe', 'power',\n",
      "       'category', 'arf', 'mileage', 'omv'],\n",
      "      dtype='object')\n",
      "nlp_train_data: Index(['title', 'description', 'features', 'accessories'], dtype='object')\n",
      "price_train_data: Index(['price'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "basic_columns = [\n",
    "    \"depreciation\",\n",
    "    \"dereg_value\",\n",
    "    \"manufactured\",\n",
    "    \"coe\",\n",
    "    \"power\",\n",
    "    \"category\",\n",
    "    \"arf\",\n",
    "    \"mileage\",\n",
    "    \"omv\",\n",
    "]\n",
    "\n",
    "nlp_columns = [\n",
    "    # \"listing_id\",\n",
    "    \"title\",\n",
    "    # \"make\",\n",
    "    # \"model\",\n",
    "    \"description\",\n",
    "    # \"manufactured\",\n",
    "    # \"original_reg_date\",\n",
    "    # \"reg_date\",\n",
    "    # \"type_of_vehicle\",\n",
    "    # \"category\",\n",
    "    # \"transmission\",\n",
    "    # \"curb_weight\",\n",
    "    # \"power\",\n",
    "    # \"fuel_type\",\n",
    "    # \"engine_cap\",\n",
    "    # \"no_of_owners\",\n",
    "    # \"depreciation\",\n",
    "    # \"coe\",\n",
    "    # \"road_tax\",\n",
    "    # \"dereg_value\",\n",
    "    # \"mileage\",\n",
    "    # \"omv\",\n",
    "    # \"arf\",\n",
    "    # \"opc_scheme\",\n",
    "    # \"lifespan\",\n",
    "    # \"eco_category\",\n",
    "    \"features\",\n",
    "    \"accessories\",\n",
    "    # \"indicative_price\",\n",
    "    # \"price\",\n",
    "]\n",
    "\n",
    "price_columns = [\n",
    "    \"price\",\n",
    "]\n",
    "\n",
    "basic_train_data = train_data[basic_columns].copy()\n",
    "nlp_train_data = train_data[nlp_columns].copy()\n",
    "price_train_data = train_data[price_columns].copy()\n",
    "\n",
    "print(f\"basic_train_data: {basic_train_data.columns}\")\n",
    "print(f\"nlp_train_data: {nlp_train_data.columns}\")\n",
    "print(f\"price_train_data: {price_train_data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(\n",
    "    data: pd.DataFrame, column_name: str, scaler: MinMaxScaler, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        data[column_name] = scaler.fit_transform(\n",
    "            data[column_name].values.reshape(-1, 1)\n",
    "        )\n",
    "    else:\n",
    "        data[column_name] = scaler.transform(data[column_name].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def standard_scaler(\n",
    "    data: pd.DataFrame, column_name: str, scaler: StandardScaler, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        data[column_name] = scaler.fit_transform(\n",
    "            data[column_name].values.reshape(-1, 1)\n",
    "        )\n",
    "    else:\n",
    "        data[column_name] = scaler.transform(data[column_name].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def binary_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: BinaryEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def onehot_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: OneHotEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def ordinal_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: OrdinalEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(pd.DataFrame(data[column_name]))\n",
    "    else:\n",
    "        labels = encoder.transform(pd.DataFrame(data[column_name]))\n",
    "    labels = labels.ravel()\n",
    "    labels = pd.DataFrame(labels, columns=[column_name])\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def target_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: TargetEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name], data[\"price\"])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def multi_label_binarizer(\n",
    "    data: pd.DataFrame,\n",
    "    column_name: str,\n",
    "    encoder: MultiLabelBinarizer,\n",
    "    refit: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    data[column_name] = data[column_name].map(\n",
    "        lambda c: [_c.strip() for _c in c.split(\",\") if _c != \"\" and _c != \"-\"]\n",
    "    )\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name]).astype(np.float64)\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name]).astype(np.float64)\n",
    "    labels = pd.DataFrame(\n",
    "        labels, columns=[column_name + \"_\" + c for c in encoder.classes_]\n",
    "    )\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "other_drop_columns = [\n",
    "    # \"make\",\n",
    "    # \"model\",\n",
    "    # \"manufactured\",\n",
    "    # \"type_of_vehicle\",\n",
    "    # \"category\",\n",
    "    # \"transmission\",\n",
    "    # \"curb_weight\",\n",
    "    # \"power\",\n",
    "    # \"fuel_type\",\n",
    "    # \"engine_cap\",\n",
    "    # \"no_of_owners\",\n",
    "    # \"depreciation\",\n",
    "    # \"coe\",\n",
    "    # \"road_tax\",\n",
    "    # \"dereg_value\",\n",
    "    # \"mileage\",\n",
    "    # \"omv\",\n",
    "    # \"arf\",\n",
    "    # \"opc_scheme\",\n",
    "    # \"price\",\n",
    "]\n",
    "\n",
    "column_encoders = {\n",
    "    # \"make\": (binary_encoder, BinaryEncoder()),\n",
    "    \"make\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    # \"model\": (binary_encoder, BinaryEncoder()),\n",
    "    \"model\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"manufactured\": (min_max_scaler, MinMaxScaler()),\n",
    "    # \"type_of_vehicle\": (binary_encoder, BinaryEncoder()),\n",
    "    \"type_of_vehicle\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"category\": (multi_label_binarizer, MultiLabelBinarizer()),\n",
    "    \"transmission\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"curb_weight\": (standard_scaler, StandardScaler()),\n",
    "    \"power\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"engine_cap\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"no_of_owners\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"depreciation\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"coe\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"road_tax\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"dereg_value\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"mileage\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"omv\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"arf\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"price\": (min_max_scaler, MinMaxScaler()),\n",
    "}\n",
    "\n",
    "def preprocess_basic_data(data: pd.DataFrame, refit: bool = False):\n",
    "    data = data.reset_index(drop=True)\n",
    "    for column_name in data.columns:\n",
    "        # print(\"====================================={}================================\".format(column_name))\n",
    "        if data[column_name].dtype != \"object\":\n",
    "            data[column_name].fillna(\n",
    "                data[column_name].mean(),\n",
    "                inplace=True,\n",
    "            )\n",
    "                \n",
    "        assert column_name in column_encoders, f\"Column {column_name} not found in column_encoders.\"\n",
    "        prefunc, encoder = column_encoders[column_name]\n",
    "        data = prefunc(data, column_name, encoder, refit)\n",
    "    return data\n",
    "\n",
    "def proprocess_nlp_data(data: pd.DataFrame):\n",
    "    BASE_TOKENIZER = \"./modelCache/mymodel/bert1/tokenizer\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_TOKENIZER, cache_dir=\"./modelCache\", clean_up_tokenization_spaces=True, local_files_only=True,)\n",
    "    data.fillna(\"\", inplace=True)\n",
    "    X_combined = data.apply(lambda x: \"|\".join(x), axis=1).values.tolist()\n",
    "    tokens = tokenizer(X_combined, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "_basic_train_data = preprocess_basic_data(basic_train_data, refit=True)\n",
    "_nlp_tokens = proprocess_nlp_data(nlp_train_data)\n",
    "_price_train_data = preprocess_basic_data(price_train_data, refit=True)\n",
    "\n",
    "basicx_train, basicx_test, nlpx_train, nlpx_test, nlpmask_train, nlpmask_test, y_train, y_test = train_test_split(\n",
    "    _basic_train_data.values,\n",
    "    _nlp_tokens['input_ids'], \n",
    "    _nlp_tokens['attention_mask'], \n",
    "    _price_train_data['price'].values,\n",
    "    test_size=0.05, random_state=21\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "modelCount = 5\n",
    "lgb_models = []\n",
    "lgb_model_dir = \"./modelCache/mymodel\"\n",
    "for i in range(modelCount):\n",
    "    model = joblib.load(f\"{lgb_model_dir}/model_{i}.pkl\")\n",
    "    lgb_models.append(model)\n",
    "\n",
    "BASE_MODEL = \"./modelCache/mymodel/bert1/model\"\n",
    "nlp_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, cache_dir=\"./modelCache\", num_labels=1, local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get light gbm prediction on train data and test data\n",
    "lgb_train_predictions = np.zeros(len(basicx_train))\n",
    "for model in lgb_models:\n",
    "    lgb_train_predictions += model.predict(basicx_train)\n",
    "lgb_train_predictions /= modelCount\n",
    "lgb_train_predictions = lgb_train_predictions.reshape(-1, 1)\n",
    "\n",
    "lgb_test_predictions = np.zeros(len(basicx_test))\n",
    "for model in lgb_models:\n",
    "    lgb_test_predictions += model.predict(basicx_test)\n",
    "lgb_test_predictions /= modelCount\n",
    "lgb_test_predictions = lgb_test_predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp train shape: torch.Size([23750, 230]), torch.Size([23750, 230])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc28a7ad371437385e1eda08301ed71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2969 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp test shape: torch.Size([1250, 230]), torch.Size([1250, 230])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ccf7212458466c80a66afc72639af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get nlp prediction on train data\n",
    "train_dataset = Dataset.from_dict({'input_ids': nlpx_train, 'attention_mask': nlpmask_train})\n",
    "print(f\"nlp train shape: {nlpx_train.shape}, {nlpmask_train.shape}\")\n",
    "train_trainer = Trainer(model=nlp_model)\n",
    "nlp_train_predictions = train_trainer.predict(train_dataset)\n",
    "nlp_train_predictions = nlp_train_predictions.predictions\n",
    "\n",
    "test_dataset = Dataset.from_dict({'input_ids': nlpx_test, 'attention_mask': nlpmask_test})\n",
    "print(f\"nlp test shape: {nlpx_test.shape}, {nlpmask_test.shape}\")\n",
    "test_trainer = Trainer(model=nlp_model)\n",
    "nlp_test_predictions = test_trainer.predict(test_dataset)\n",
    "nlp_test_predictions = nlp_test_predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate rmse: 21977.465062154468.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./modelCache/mymodel/meta_model.pkl']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a linear model based on the 2 precitions\n",
    "combined_train_data = np.concatenate((lgb_train_predictions, nlp_train_predictions), axis=1)\n",
    "combined_test_data = np.concatenate((lgb_test_predictions, nlp_test_predictions), axis=1)\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(combined_train_data, y_train)\n",
    "\n",
    "combined_test_pred = meta_model.predict(combined_test_data)\n",
    "combined_test_pred = column_encoders[\"price\"][1].inverse_transform(combined_test_pred.reshape(-1, 1))\n",
    "actual_test_pred = column_encoders[\"price\"][1].inverse_transform(y_test.reshape(-1, 1))\n",
    "rmse = np.sqrt(mean_squared_error(actual_test_pred, combined_test_pred))\n",
    "print(f\"evaluate rmse: {rmse}.\")\n",
    "\n",
    "joblib.dump(meta_model, f\"{lgb_model_dir}/meta_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate result_4.csv for submission: prepare the data\n",
    "basic_sub_data = test_data[basic_columns].copy()\n",
    "nlp_sub_data = test_data[nlp_columns].copy()\n",
    "basic_sub_data = preprocess_basic_data(basic_sub_data)\n",
    "nlp_sub_tokens = proprocess_nlp_data(nlp_sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate result_4.csv for submission: get lightgbm prediction\n",
    "lgb_sub_predictions = np.zeros(len(basic_sub_data))\n",
    "for model in lgb_models:\n",
    "    lgb_sub_predictions += model.predict(basic_sub_data)\n",
    "lgb_sub_predictions /= modelCount\n",
    "lgb_sub_predictions = lgb_sub_predictions.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp data shape: torch.Size([10000, 240]), torch.Size([10000, 240])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fc94718619471fab3d6850a0a53403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate result_4.csv for submission: get nlp prediction\n",
    "nlp_sub_data = nlp_sub_tokens['input_ids']\n",
    "nlp_sub_mask = nlp_sub_tokens['attention_mask']\n",
    "train_dataset = Dataset.from_dict({'input_ids': nlp_sub_data, 'attention_mask': nlp_sub_mask})\n",
    "print(f\"nlp data shape: {nlp_sub_data.shape}, {nlp_sub_mask.shape}\")\n",
    "sub_trainer = Trainer(model=nlp_model)\n",
    "nlp_sub_predictions = sub_trainer.predict(train_dataset)\n",
    "nlp_sub_predictions = nlp_sub_predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate result_4.csv for submission: get final prediction\n",
    "combined_sub_data = np.concatenate((lgb_sub_predictions, nlp_sub_predictions), axis=1)\n",
    "\n",
    "sub_meta_model = joblib.load(f\"{lgb_model_dir}/meta_model.pkl\")\n",
    "\n",
    "combined_sub_pred = sub_meta_model.predict(combined_sub_data)\n",
    "combined_sub_pred = column_encoders[\"price\"][1].inverse_transform(combined_sub_pred.reshape(-1, 1))\n",
    "res_df = pd.DataFrame({\"Id\": range(0, combined_sub_pred.shape[0]), \"Predicted\": combined_sub_pred.ravel()})\n",
    "res_df.to_csv('result_4.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_lgb: 22074.994318534722, rmse_nlp: 28886.207178926772\n",
      "rmse_combined: 21977.465062154468\n"
     ]
    }
   ],
   "source": [
    "check_gt = column_encoders[\"price\"][1].inverse_transform(y_train.reshape(-1, 1))\n",
    "check_lgb_pred = column_encoders[\"price\"][1].inverse_transform(lgb_train_predictions)\n",
    "check_nlp_pred = column_encoders[\"price\"][1].inverse_transform(nlp_train_predictions)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(check_gt, check_lgb_pred))\n",
    "rmse_nlp = np.sqrt(mean_squared_error(check_gt, check_nlp_pred))\n",
    "print(f\"rmse_lgb: {rmse_lgb}, rmse_nlp: {rmse_nlp}\")\n",
    "print(f\"rmse_combined: {rmse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
