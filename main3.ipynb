{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from category_encoders import BinaryEncoder, OneHotEncoder, TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OrdinalEncoder\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "from datasets import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data number = 25000\n",
      "Test data number = 10000\n",
      "\n",
      "Index(['listing_id', 'title', 'make', 'model', 'description', 'manufactured',\n",
      "       'original_reg_date', 'reg_date', 'type_of_vehicle', 'category',\n",
      "       'transmission', 'curb_weight', 'power', 'fuel_type', 'engine_cap',\n",
      "       'no_of_owners', 'depreciation', 'coe', 'road_tax', 'dereg_value',\n",
      "       'mileage', 'omv', 'arf', 'opc_scheme', 'lifespan', 'eco_category',\n",
      "       'features', 'accessories', 'indicative_price', 'price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train_data = pd.read_csv('Data/train.csv')\n",
    "test_data = pd.read_csv(\"Data/test.csv\")\n",
    "\n",
    "print('Training data number = {}'.format(train_data.shape[0]))\n",
    "print('Test data number = {}\\n'.format(test_data.shape[0]))\n",
    "print(train_data.columns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'description', 'features', 'accessories', 'price'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "basic_drop_columns = [\n",
    "    \"listing_id\",\n",
    "    # \"title\",\n",
    "    \"make\",\n",
    "    \"model\",\n",
    "    # \"description\",\n",
    "    \"manufactured\",\n",
    "    \"original_reg_date\",\n",
    "    \"reg_date\",\n",
    "    \"type_of_vehicle\",\n",
    "    \"category\",\n",
    "    \"transmission\",\n",
    "    \"curb_weight\",\n",
    "    \"power\",\n",
    "    \"fuel_type\",\n",
    "    \"engine_cap\",\n",
    "    \"no_of_owners\",\n",
    "    \"depreciation\",\n",
    "    \"coe\",\n",
    "    \"road_tax\",\n",
    "    \"dereg_value\",\n",
    "    \"mileage\",\n",
    "    \"omv\",\n",
    "    \"arf\",\n",
    "    \"opc_scheme\",\n",
    "    \"lifespan\",\n",
    "    \"eco_category\",\n",
    "    # \"features\",\n",
    "    # \"accessories\",\n",
    "    \"indicative_price\",\n",
    "    # \"price\",\n",
    "]\n",
    "_train_data = train_data.drop(columns=basic_drop_columns)\n",
    "print(_train_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def min_max_scaler(\n",
    "    data: pd.DataFrame, column_name: str, scaler: MinMaxScaler, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        data[column_name] = scaler.fit_transform(\n",
    "            data[column_name].values.reshape(-1, 1)\n",
    "        )\n",
    "    else:\n",
    "        data[column_name] = scaler.transform(data[column_name].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def standard_scaler(\n",
    "    data: pd.DataFrame, column_name: str, scaler: StandardScaler, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        data[column_name] = scaler.fit_transform(\n",
    "            data[column_name].values.reshape(-1, 1)\n",
    "        )\n",
    "    else:\n",
    "        data[column_name] = scaler.transform(data[column_name].values.reshape(-1, 1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def binary_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: BinaryEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def onehot_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: OneHotEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def ordinal_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: OrdinalEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(pd.DataFrame(data[column_name]))\n",
    "    else:\n",
    "        labels = encoder.transform(pd.DataFrame(data[column_name]))\n",
    "    labels = labels.ravel()\n",
    "    labels = pd.DataFrame(labels, columns=[column_name])\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def target_encoder(\n",
    "    data: pd.DataFrame, column_name: str, encoder: TargetEncoder, refit: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name], data[\"price\"])\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name])\n",
    "    labels = pd.DataFrame(labels)\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "def multi_label_binarizer(\n",
    "    data: pd.DataFrame,\n",
    "    column_name: str,\n",
    "    encoder: MultiLabelBinarizer,\n",
    "    refit: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    data[column_name] = data[column_name].map(\n",
    "        lambda c: [_c.strip() for _c in c.split(\",\") if _c != \"\" and _c != \"-\"]\n",
    "    )\n",
    "    if refit:\n",
    "        labels = encoder.fit_transform(data[column_name]).astype(np.float64)\n",
    "    else:\n",
    "        labels = encoder.transform(data[column_name]).astype(np.float64)\n",
    "    labels = pd.DataFrame(\n",
    "        labels, columns=[column_name + \"_\" + c for c in encoder.classes_]\n",
    "    )\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return pd.concat([data, labels], axis=1)\n",
    "\n",
    "\n",
    "other_drop_columns = [\n",
    "    # \"make\",\n",
    "    # \"model\",\n",
    "    # \"manufactured\",\n",
    "    # \"type_of_vehicle\",\n",
    "    # \"category\",\n",
    "    # \"transmission\",\n",
    "    # \"curb_weight\",\n",
    "    # \"power\",\n",
    "    # \"fuel_type\",\n",
    "    # \"engine_cap\",\n",
    "    # \"no_of_owners\",\n",
    "    # \"depreciation\",\n",
    "    # \"coe\",\n",
    "    # \"road_tax\",\n",
    "    # \"dereg_value\",\n",
    "    # \"mileage\",\n",
    "    # \"omv\",\n",
    "    # \"arf\",\n",
    "    # \"opc_scheme\",\n",
    "    # \"price\",\n",
    "]\n",
    "\n",
    "column_encoders = {\n",
    "    # \"make\": (binary_encoder, BinaryEncoder()),\n",
    "    \"make\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    # \"model\": (binary_encoder, BinaryEncoder()),\n",
    "    \"model\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"manufactured\": (min_max_scaler, MinMaxScaler()),\n",
    "    # \"type_of_vehicle\": (binary_encoder, BinaryEncoder()),\n",
    "    \"type_of_vehicle\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"category\": (multi_label_binarizer, MultiLabelBinarizer()),\n",
    "    \"transmission\": (ordinal_encoder, OrdinalEncoder()),\n",
    "    \"curb_weight\": (standard_scaler, StandardScaler()),\n",
    "    \"power\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"engine_cap\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"no_of_owners\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"depreciation\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"coe\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"road_tax\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"dereg_value\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"mileage\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"omv\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"arf\": (min_max_scaler, MinMaxScaler()),\n",
    "    \"price\": (min_max_scaler, MinMaxScaler()),\n",
    "}\n",
    "\n",
    "\n",
    "_train_data = _train_data.drop(columns=other_drop_columns)\n",
    "_train_data = _train_data.dropna()\n",
    "_train_data = column_encoders[\"price\"][0](\n",
    "    _train_data, \"price\", column_encoders[\"price\"][1], refit=True\n",
    ")\n",
    "\n",
    "# BASE_MODEL = \"camembert-base\"\n",
    "BASE_MODEL = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, cache_dir=\"./modelCache\", clean_up_tokenization_spaces=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASE_MODEL, cache_dir=\"./modelCache\", num_labels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = _train_data[[\"title\", \"description\", \"features\", \"accessories\"]]\n",
    "Y = _train_data[\"price\"]\n",
    "\n",
    "X_combined = X.apply(lambda x: \"|\".join(x), axis=1).values.tolist()\n",
    "tokens = tokenizer(X_combined, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "x_train, x_test, y_train, y_test, mask_train, mask_test = train_test_split(tokens['input_ids'], torch.Tensor(Y.values), tokens['attention_mask'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16771, 230]) torch.Size([16771]) torch.Size([16771, 230])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afb49befd874ed9a31e6474bf19f398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0081, 'grad_norm': 0.3073815405368805, 'learning_rate': 1.9761564139246544e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0023, 'grad_norm': 0.3855680823326111, 'learning_rate': 1.9523128278493086e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0019, 'grad_norm': 0.23669053614139557, 'learning_rate': 1.928469241773963e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0017, 'grad_norm': 0.28194108605384827, 'learning_rate': 1.904625655698617e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fa665f86024c91af482d5bebc54395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0017062259139493108, 'eval_rmse': 119735.078125, 'eval_runtime': 32.0583, 'eval_samples_per_second': 130.793, 'eval_steps_per_second': 16.376, 'epoch': 1.0}\n",
      "{'loss': 0.0011, 'grad_norm': 0.39601826667785645, 'learning_rate': 1.8807820696232715e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0013, 'grad_norm': 0.2727905213832855, 'learning_rate': 1.8569384835479257e-05, 'epoch': 1.43}\n",
      "{'loss': 0.001, 'grad_norm': 0.27829667925834656, 'learning_rate': 1.83309489747258e-05, 'epoch': 1.67}\n",
      "{'loss': 0.001, 'grad_norm': 0.549113392829895, 'learning_rate': 1.8092513113972344e-05, 'epoch': 1.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b487753b6c4c0381a4770cf81d0c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.000999927637167275, 'eval_rmse': 91661.59375, 'eval_runtime': 32.8501, 'eval_samples_per_second': 127.64, 'eval_steps_per_second': 15.982, 'epoch': 2.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 40\u001b[0m\n\u001b[0;32m     19\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m     20\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./modelCache/mymodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mLEARNING_RATE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     greater_is_better\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_for_regression,\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\application\\anaconda3\\envs\\work\\lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\application\\anaconda3\\envs\\work\\lib\\site-packages\\transformers\\trainer.py:2390\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_dict({'input_ids': x_train, 'label': y_train, 'attention_mask': mask_train})\n",
    "test_dataset = Dataset.from_dict({'input_ids': x_test, 'label': y_test, 'attention_mask': mask_test})\n",
    "\n",
    "print(x_train.shape, y_train.shape, mask_train.shape)\n",
    "\n",
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    \n",
    "    logits = column_encoders[\"price\"][1].inverse_transform(logits)\n",
    "    labels = column_encoders[\"price\"][1].inverse_transform(labels)\n",
    "    rmse = np.sqrt(mean_squared_error(labels, logits))\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 20\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./modelCache/mymodel\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"rmse\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    greater_is_better=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics_for_regression,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"./model/mytokenizer.json\")\n",
    "trainer.save_model(\"./model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
